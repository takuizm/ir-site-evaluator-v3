#!/usr/bin/env python
"""ãƒãƒƒãƒå®Ÿè¡Œçµæœã‚’çµ±åˆ

å…¨ãƒãƒƒãƒã®çµæœCSVã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±åˆã—ã¾ã™ã€‚
"""
import pandas as pd
import glob
from pathlib import Path
from datetime import datetime

def merge_results(
    pattern: str = "output/batch_*_results.csv",
    detailed_pattern: str = "output/batch_*_detailed.csv",
    output_dir: str = "output"
):
    """ãƒãƒƒãƒå®Ÿè¡Œçµæœã‚’çµ±åˆ

    Args:
        pattern: ã‚µãƒãƒªãƒ¼CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ³¨: å®Ÿéš›ã¯å…¨ãƒã‚§ãƒƒã‚¯çµæœCSVï¼‰
        detailed_pattern: è©³ç´°CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ³¨: å®Ÿéš›ã¯ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼CSVï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # å…¨ãƒã‚§ãƒƒã‚¯çµæœã®çµ±åˆï¼ˆbatch_*_results.csvï¼‰
    print("ğŸ“Š Merging all validation results...")
    result_files = sorted(glob.glob(pattern))

    if not result_files:
        print(f"âŒ No result files found matching: {pattern}")
        return

    all_results = []
    for batch_file in result_files:
        df = pd.read_csv(batch_file, low_memory=False)
        all_results.append(df)
        print(f"  âœ“ {batch_file}: {len(df)} validation checks")

    results_df = pd.concat(all_results, ignore_index=True)
    results_output = output_path / f"final_all_results_{timestamp}.csv"
    results_df.to_csv(results_output, index=False, encoding='utf-8-sig')

    print(f"\nâœ… All validation results merged: {results_output}")
    print(f"   Total validation checks: {len(results_df):,}")
    print(f"   Unique sites: {results_df['site_id'].nunique()}")

    # ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼ã®çµ±åˆï¼ˆbatch_*_detailed.csvï¼‰
    print("\nğŸ“‹ Merging site summaries...")
    detailed_files = sorted(glob.glob(detailed_pattern))

    if not detailed_files:
        print(f"âš ï¸  No detailed files found matching: {detailed_pattern}")
        detailed_df = None
    else:
        all_details = []
        for batch_file in detailed_files:
            df = pd.read_csv(batch_file)
            all_details.append(df)
            print(f"  âœ“ {batch_file}: {len(df)} sites")

        detailed_df = pd.concat(all_details, ignore_index=True)
        detailed_output = output_path / f"final_site_summary_{timestamp}.csv"
        detailed_df.to_csv(detailed_output, index=False, encoding='utf-8-sig')

        print(f"\nâœ… Site summaries merged: {detailed_output}")
        print(f"   Total rows (site Ã— category): {len(detailed_df)}")
        print(f"   Unique sites: {detailed_df['site_id'].nunique()}")

    # çµ±è¨ˆæƒ…å ±
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Statistics")
    print("=" * 60)

    # ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼ã‹ã‚‰çµ±è¨ˆï¼ˆbatch_*_detailed.csvï¼‰
    if detailed_df is not None and 'pass_count' in detailed_df.columns:
        total_pass = detailed_df['pass_count'].sum()
        total_fail = detailed_df['fail_count'].sum()
        total_unknown = detailed_df.get('unknown_count', pd.Series([0])).sum()
        total_error = detailed_df.get('error_count', pd.Series([0])).sum()
        total_not_supported = detailed_df.get('not_supported_count', pd.Series([0])).sum()
        total_checks = detailed_df['total_items'].sum()

        print(f"Total Sites: {detailed_df['site_id'].nunique()}")
        print(f"Total Checks: {total_checks:,}")
        print(f"\nResults by count:")
        print(f"  PASS:           {total_pass:6,} ({total_pass/total_checks*100:5.2f}%)")
        print(f"  FAIL:           {total_fail:6,} ({total_fail/total_checks*100:5.2f}%)")
        print(f"  UNKNOWN:        {total_unknown:6,} ({total_unknown/total_checks*100:5.2f}%)")
        print(f"  ERROR:          {total_error:6,} ({total_error/total_checks*100:5.2f}%)")
        print(f"  NOT_SUPPORTED:  {total_not_supported:6,} ({total_not_supported/total_checks*100:5.2f}%)")

    # è©³ç´°çµæœåˆ¥ã‚«ã‚¦ãƒ³ãƒˆï¼ˆbatch_*_results.csvã‹ã‚‰ï¼‰
    if 'result' in results_df.columns:
        print(f"\nğŸ“Š Detailed Result Distribution:")
        result_counts = results_df['result'].value_counts()
        total = len(results_df)
        for result, count in result_counts.items():
            percentage = count / total * 100
            print(f"  {result:15s}: {count:6,} ({percentage:5.2f}%)")

    print("\nğŸ‰ Merge completed!")
    print(f"\nğŸ“ Output files:")
    print(f"  All Results: {results_output}")
    if detailed_files:
        print(f"  Site Summary: {detailed_output}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ãƒãƒƒãƒå®Ÿè¡Œçµæœã‚’çµ±åˆ")
    parser.add_argument(
        "--pattern",
        default="output/batch_*_results.csv",
        help="ã‚µãƒãƒªãƒ¼CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output/batch_*_results.csvï¼‰"
    )
    parser.add_argument(
        "--detailed-pattern",
        default="output/batch_*_detailed.csv",
        help="è©³ç´°CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output/batch_*_detailed.csvï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        default="output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputï¼‰"
    )

    args = parser.parse_args()

    merge_results(args.pattern, args.detailed_pattern, args.output_dir)
